{{ if karpenterNodePools .Cluster.NodePools }}
{{- with $data := . }}
{{- range $nodePool := .Cluster.NodePools }}
{{- if eq $nodePool.Profile "worker-karpenter" }}
---
apiVersion: karpenter.sh/v1alpha5
kind: Provisioner
metadata:
  name: "{{$nodePool.Name}}"
spec:
  {{- if index $nodePool.ConfigItems "scaling_priority"}}
  weight: {{$nodePool.ConfigItems.scaling_priority}}
  {{- end}}
  consolidation:
    enabled: true
  requirements:
    - key: "node.kubernetes.io/instance-type"
      operator: In
      values:
{{- range $type := $nodePool.InstanceTypes }}
      - "{{ $type }}"
{{- end }}
    - key: karpenter.sh/capacity-type
      operator: In
      values: [{{if $nodePool.IsSpot}}"spot"{{else}}"on-demand"{{end}}]
    - key: "kubernetes.io/arch"
      operator: In
      values: ["arm64", "amd64"]
#      values: ["amd64"]
    - key: "topology.kubernetes.io/zone"
      operator: In
      values:
# TODO: handle per node pool availability zone limits
{{- range $az := $data.Values.availability_zones }}
      - "{{ $az }}"
{{- end }}
  # limits:
  #   resources:
  #     cpu: 1000
{{- if index $nodePool.ConfigItems "taints"}}
  taints:
  {{- range split $nodePool.ConfigItems.taints ","}}
    {{- $taint := split . "="}}
      - key: {{index $taint 0}}
        effect: {{index $taint 1}}
  {{- end}}
{{- end}}

  startupTaints:
  - key: zalando.org/node-not-ready
    effect: NoSchedule

  labels:
    # these labels are normally set by kubelet on start-up, but because
    # karpenter already creates the node object ahead of the instance start, we
    # need to set them here as well.
    lifecycle-status: ready
    node.kubernetes.io/node-pool: "{{ $nodePool.Name }}"
    node.kubernetes.io/role: worker
    node.kubernetes.io/profile: "{{ $nodePool.Profile }}"
    cluster-lifecycle-controller.zalan.do/replacement-strategy: none
{{- if index $nodePool.ConfigItems "labels"}}
  {{- range split $nodePool.ConfigItems.labels ","}}
    {{- $label := split . "="}}
    {{index $label 0}}: {{index $label 1}}
  {{- end}}
{{- end}}

  # ttlSecondsAfterEmpty: 30

  provider:
    launchTemplate: "{{ $data.Cluster.ID | awsValidID }}-{{ $nodePool.Name }}"
    subnetSelector:
      Name: "dmz-eu-central-1*" # TODO: use a better tag
    # securityGroupSelector:
    #   karpenter.sh/discovery: ${CLUSTER_NAME}
    tags:
      InfrastructureComponent: "true"
      # TODO: maybe use application label on dedicated node pools?
      application: kubernetes
      component: worker
      environment: "{{ $data.Cluster.Environment }}"
      Name: "{{ $nodePool.Name }} ({{ $data.Cluster.ID }})"
      # TODO: are these tags still needed?
      # Do we want a node pool tag for cost tracking reasons?
      node.kubernetes.io/role: worker
      node.kubernetes.io/node-pool: "{{ $nodePool.Name }}"
# only node pools without taints should be attached to Ingress Load balancer
{{- if or (not (index $nodePool.ConfigItems "taints")) (eq (index $nodePool.ConfigItems "taints") "dedicated=skipper-ingress:NoSchedule") }}
      zalando.org/ingress-enabled: "true"
{{- end }}
      'zalando.de/cluster-local-id/{{ $data.Cluster.LocalID }}': owned
      zalando.org/pod-max-pids: "{{ $nodePool.ConfigItems.pod_max_pids }}"
{{- end }}
{{- end }}
{{- end }}
{{- end }}
