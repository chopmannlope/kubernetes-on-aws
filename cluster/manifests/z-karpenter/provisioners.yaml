{{ if karpenterNodePools .Cluster.NodePools }}
{{- with $data := . }}
{{- range $nodePool := .Cluster.NodePools }}
{{- if eq $nodePool.Profile "worker-karpenter" }}
{{- range $capacityType := capacityTypes }}
---
apiVersion: karpenter.sh/v1alpha5
kind: Provisioner
metadata:
  name: "{{$nodePool.Name}}-{{$capacityType}}"
spec:
  {{- if index $nodePool.ConfigItems "scaling_priority"}}
  weight: {{$nodePool.ConfigItems.scaling_priority}}
  {{- end}}
  consolidation:
    enabled: true
  requirements:
    - key: "node.kubernetes.io/instance-type"
      operator: In
      values:
{{- range $type := $nodePool.InstanceTypes }}
      - "{{ $type }}"
{{- end }}
    - key: karpenter.sh/capacity-type
      operator: In
      values: [{{if eq $capacityType "spot"}}"spot", {{end}}"on-demand"]

    - key: "kubernetes.io/arch"
      operator: In
      values: ["arm64", "amd64"]

    - key: "topology.kubernetes.io/zone"
      operator: In
      values:

# TODO: handle per node pool availability zone limits
{{- $azs := $data.Values.availability_zones}}
{{- if index $nodePool.ConfigItems "availability_zones"}}
        {{    $azs = split $nodePool.ConfigItems.availability_zones "," }}
{{- end}}
{{- range $az :=  $azs }}
      - "{{ $az }}"
{{- end }}

    # capacity spread constraints
    # TODO: template function?
    {{- if eq $capacityType "spot"}}
    - key: "capacity.spread.1-9" # 10% spot
      operator: In
      values: ["1"]

    - key: "capacity.spread.1-7" # 12.5% spot
      operator: In
      values: ["1"]

    - key: "capacity.spread.1-4" # 20% spot
      operator: In
      values: ["1"]

    - key: "capacity.spread.1-3" # 25% spot
      operator: In
      values: ["1"]

    - key: "capacity.spread.1-2" # 33.3% spot
      operator: In
      values: ["1"]

    - key: "capacity.spread.1-2" # 50% spot
      operator: In
      values: ["1"]

    - key: "capacity.spread.3-2" # 60% spot
      operator: In
      values: ["1", "2", "3"]

    - key: "capacity.spread.3-1" # 75% spot
      operator: In
      values: ["1", "2", "3"]

    - key: "capacity.spread.4-1" # 80% spot
      operator: In
      values: ["1", "2", "3", "4"]

    - key: "capacity.spread.9-1" # 90% spot
      operator: In
      values: ["1", "2", "3", "4", "5", "6", "7", "8", "9"]
    {{- else}}
    - key: "capacity.spread.1-9" # 10% spot
      operator: In
      values: ["2", "3", "4", "5", "6", "7", "8", "9", "10"]

    - key: "capacity.spread.1-7" # 12.5% spot
      operator: In
      values: ["2", "3", "4", "5", "6", "7", "8"]

    - key: "capacity.spread.1-4" # 20% spot
      operator: In
      values: ["2", "3", "4", "5"]

    - key: "capacity.spread.1-3" # 25% spot
      operator: In
      values: ["2", "3", "4"]

    - key: "capacity.spread.1-2" # 33.3% spot
      operator: In
      values: ["2", "3"]

    - key: "capacity.spread.3-2" # 60% spot
      operator: In
      values: ["4", "5"]

    - key: "capacity.spread.3-1" # 75% spot
      operator: In
      values: ["4"]

    - key: "capacity.spread.4-1" # 80% spot
      operator: In
      values: ["5"]

    - key: "capacity.spread.9-1" # 90% spot
      operator: In
      values: ["10"]
    {{- end}}
  # limits:
  #   resources:
  #     cpu: 1000
{{- $taints := $nodePool.Taints}}
{{- if $taints }}
  taints:
  {{- range $taints }}
    {{- $taint := . }}
      - key: {{$taint.Key}}
        effect: {{$taint.Effect}}
        {{- if $taint.Value }}
        value: {{$taint.Value }}
        {{- end }}
  {{- end}}
{{- end}}

  startupTaints:
  - key: zalando.org/node-not-ready
    effect: NoSchedule

  labels:
    # these labels are normally set by kubelet on start-up, but because
    # karpenter already creates the node object ahead of the instance start, we
    # need to set them here as well.
    lifecycle-status: ready
    node.kubernetes.io/node-pool: "{{ $nodePool.Name }}"
    node.kubernetes.io/role: worker
    node.kubernetes.io/profile: "{{ $nodePool.Profile }}"
    cluster-lifecycle-controller.zalan.do/replacement-strategy: none
{{- if index $nodePool.ConfigItems "labels"}}
  {{- range split $nodePool.ConfigItems.labels ","}}
    {{- $label := split . "="}}
    {{index $label 0}}: {{index $label 1}}
  {{- end}}
{{- end}}

  # ttlSecondsAfterEmpty: 30

  provider:
    launchTemplate: "{{ $data.Cluster.ID | awsValidID }}-{{ $nodePool.Name }}"
    subnetSelector:
      Name: "dmz-eu-central-1*" # TODO: use a better tag
    # securityGroupSelector:
    #   karpenter.sh/discovery: ${CLUSTER_NAME}
    tags:
      InfrastructureComponent: "true"
      # TODO: maybe use application label on dedicated node pools?
      application: kubernetes
      component: worker
      environment: "{{ $data.Cluster.Environment }}"
      Name: "{{ $nodePool.Name }} ({{ $data.Cluster.ID }})"
      # TODO: are these tags still needed?
      # Do we want a node pool tag for cost tracking reasons?
      node.kubernetes.io/role: worker
      node.kubernetes.io/node-pool: "{{ $nodePool.Name }}"
# only node pools without taints should be attached to Ingress Load balancer
{{- if or (not (index $nodePool.ConfigItems "taints")) (eq (index $nodePool.ConfigItems "taints") "dedicated=skipper-ingress:NoSchedule") }}
      zalando.org/ingress-enabled: "true"
{{- end }}
      'zalando.de/cluster-local-id/{{ $data.Cluster.LocalID }}': owned
      zalando.org/pod-max-pids: "{{ $nodePool.ConfigItems.pod_max_pids }}"
{{- end }}
{{- end }}
{{- end }}
{{- end }}
{{- end }}
